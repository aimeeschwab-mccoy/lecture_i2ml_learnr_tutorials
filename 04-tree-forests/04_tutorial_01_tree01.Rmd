## Tree

### Study Goals

*Theoretical (T)*

- Get to know the idea of CART

*Practical (P)*

- Learn how to train a model for classification or regression with CART using `mlr`

### Preparation

1.  *(T)* Watch the following video  (sorry, rather low volume...):
    <center>
![](https://www.youtube.com/watch?v=xZRyi2xuCd0&list=PLMyWaJl2LoXyhFvMjtbBGs0Pi8khHbKm3&index=2&t=0s){width="75%"}
    </center>

### Exercises

#### *(T)* Quiz

```{r tree-quiz1, echo=FALSE}
  question("Which statements about CART are true?",
    answer("The prediction function defined by a CART divides the target variable into colored rectangles"),
    answer("The prediction function defined by a CART can be described in terms of a nested series of if-else statements about feature values.", correct = TRUE),
    answer("The prediction function defined by a CART divides the feature space into disjoint rectangles", correct = TRUE),
    answer("To find optimal splits, one iterates over all features, and for each feature over all possible split points.", correct = TRUE),
    answer("To find optimal splits, we use the one that splits the data approximately in half in each step."),
    answer("To find optimal splits, we evaluate the possible splits only on the data that ended up in the parent node we are trying to split.", correct = TRUE),
    answer("The optimal split results in the lowest child node impurity of all possible splits.",correct = TRUE),
    answer("Entropy and Gini index are more sensitive to how pure a node is than misclassification error.",correct = TRUE),
    answer("For metric variables, the optimal split point is often not unique in the sense that all values in a certain interval yield the same split of observations in the child nodes.",correct = TRUE)
)
```

#### *(P)* Create the `mlr` learner

Create a task for classification using the following data:

```{r tree-definition, exercise=TRUE, exercise.lines=5, exercise.checker=createChecker("spirals_task")}
set.seed(1337)
spirals = mlbench.spirals(500, sd = 0.1)
spirals = as.data.frame(spirals)
spirals_task = 
```

```{r tree-definition-hint-1, eval=FALSE}
# Use the 'makeClassifTask' function of mlr
makeClassifTask(...)
```


```{r tree-definition-check}
set.seed(1337)
spirals = mlbench.spirals(500, sd = 0.1)
spirals = as.data.frame(spirals)
spirals_task = makeClassifTask(data = spirals, target = "classes")
```


#### *(P)* Train the `mlr` learner

Now train the learner on the spirals_task:
```{r tree-definition1, exercise=TRUE, exercise.lines=5, exercise.checker=createChecker("tree_learner")}
tree_learner =
```

```{r tree-definition1-hint-1, eval=FALSE}
# Use the 'makeLearner' function of mlr
makeLearner(...)
```

```{r tree-definition1-check}
tree_learner = makeLearner("classif.rpart")
```


#### *(P)* Visualize decision boundaries

Again, define the `tree_learner` and visualize the prediction of the learner with `plotLearnerPrediction`. Rerun the code for different sizes of tree `md`. What can you observe by varying the hyperparameter?

```{r, include=FALSE}
set.seed(1337)
spirals = mlbench.spirals(500, sd = 0.1)
spirals = as.data.frame(spirals)
spirals_task = makeClassifTask(data = spirals, target = "classes")
tree_learner = makeLearner("classif.rpart")
```

```{r tree-definition-visualization, exercise=TRUE}
maxdepths = c(1, 3, 5, 7)

for (maxdepth in maxdepths) {
  print(plotLearnerPrediction(learner =  , task =  , maxdepth = maxdepth))
  BBmisc::pause()
}
```
