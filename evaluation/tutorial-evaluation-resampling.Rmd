## Resampling

```{r, include=FALSE}
resampleChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{
  msg = taskChecker("task")(label, user_code, check_code, envir_result, evaluate_result)
  if (! is.null(msg))
    return(msg)

  setup_state(sol_code = check_code, stu_code = user_code)

  msg = errorToMessage(expr = {
    ex() %>% check_object("learner") %>% check_equal()
    ex() %>% check_object("res_desc") %>% check_equal()
    ex() %>% check_object("res") %>% check_element("measures.train") %>% check_equal()
    ex() %>% check_object("res") %>% check_element("measures.test") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}

benchmarkChecker = function (label, user_code, check_code, envir_result, evaluate_result, ...)
{

  add_code = "
  df_bmr = as.data.frame(bmr)
  df_bmr$learner.id = as.character(df_bmr$learner.id)
  df_bmr = df_bmr[with(df_bmr, order(learner.id, mmce)), -which(names(df_bmr) %in% c(\"task.id\", \"iter\"))]
  attr(df_bmr, \"row.names\") = seq_len(nrow(df_bmr))
  "

  setup_state(sol_code = paste0(check_code, add_code), stu_code = paste0(user_code, add_code))

  msg = errorToMessage(expr = {
    ex() %>% check_object("learners")
    ex() %>% check_object("task")
    ex() %>% check_object("res_desc") %>% check_equal()
    ex() %>% check_object("bmr")
    ex() %>% check_object("df_bmr") %>% check_equal()
  })
  if (! is.null(msg))
    return(msg)

  return(list(message = "Great job! :)", correct = TRUE, location = "append"))
}
```

### Study Goals

*Theoretical (T)*

- Know the goals and the idea of resampling
- Get familiar with cross-validation, bootstrapping, and subsampling

*Practical (P)*

- Learn how to define and conduct a resampling strategy with `mlr`
- Find out how to apply `benchmark` experiments
- Interprete and visualize the benchmark result

### Preparation

1.  *(T)* Watch the following video  (sorry, rather low volume...):
    <center>
    ![](https://youtu.be/NvDUk8Bxuho){width="75%"}
    </center>
    <!-- <a href="https://github.com/compstat-lmu/lecture_i2ml/blob/master/slides-pdf/slides-resampling.pdf" target="_blank">Slideset</a> -->

1. *(P)* Read the `mlr` tutorial about [resampling](https://mlr.mlr-org.com/articles/tutorial/resample.html)

### Exercises

#### *(T)* Quiz

```{r resampling-quiz, echo=FALSE}
  question("Which statements are true?",
    answer("The method of resampling is a nonparametric method of statistical inference.", correct = TRUE),
    answer("Subsampling is basically a repeated holdout and is also very similar to
           bootstrapping, so subsampling is done with replacement."),
    answer("Estimating the expected generalization error is a goal of resampling.", correct = TRUE),
    answer("In Resampling, the data set is split repeatedly into training and tests
          sets.", correct = TRUE)

  )
```

#### *(P)* Define a resampling strategy

For this exercise we want to evaluate the learner with 10-fold cross-validation. Define the resampling using the function `makeResampleDesc()`:

```{r resample-desc, exercise=TRUE, exercise.lines=5, exercise.checker=createChecker("res_desc")}
res_desc = makeResampleDesc(...)
```

```{r resample-desc-hint-1}
# You can specify the 'method' in 'makeResampleDesc()' for different strategies (see '?makeResampleDesc'). For cross validation for instance:
method = "CV"
```

```{r resample-desc-hint-2}
# The parameter are also listed in the help page. To define how many folds use the 'iters' argument:
iters = 10
```

```{r resample-desc-solution}
res_desc = makeResampleDesc(method = "CV", iters = 10)
```

```{r resample-desc-check}
res_desc = makeResampleDesc(method = "CV", iters = 10)
```

#### *(P)* Conduct resampling

Now it is time to evaluate the LDA learner (`"classif.lda`") with 10-fold cross-validation on the iris task (`iris.task`). Therefore, use the `resample()` function and specify the learner, task, and the resampling (the seed is required for checking your result, please let it as `123`, otherwise your results cannot be checked):

```{r resample, exercise=TRUE, exercise.lines=10, exercise.checker=resampleChecker}
learner =
task =
res_desc =

set.seed(123)
res = resample(...)
```

```{r resample-hint-1}
# The learner can be specified as character or via `makeLearner`
learner = "classif.lda"
learner = makeLearner(classif.lda)
```

```{r resample-hint-2}
# For the task use the build in `iris.task` or define it by yourself
task = iris.task
task = makeClassifTask(data = iris, target = "Species")
```

```{r resample-hint-3}
# As resampling description use the 10-fold cross validation:
res_desc = cv10
res_desc = makeResampleDesc(method = "CV", iters = 10)
```

```{r resample-hint-4}
# Finally, use resample on all the components to evaluate the learner
res = resample(learner = learner, task = task, resampling = res_desc)
```

```{r resample-solution}
learner = "classif.lda"
task = iris.task
res_desc = cv10

set.seed(123)
res = resample(learner = learner, task = task, resampling = res_desc)
```

```{r resample-check}
learner = "classif.lda"
task = iris.task
res_desc = cv10

set.seed(123)
res = resample(learner = learner, task = task, resampling = res_desc)
```

**Note:** The performance measure can also be specified. But in this example we are using the default measure which is, for multiclass classification, the mean misclassification error.

```{r make-resample-desc-quiz, echo=FALSE}
question("What might be the difference of using build in resample objects like 'cv10' or defining them with 'makeResampleDesc()'?",
  answer("There is no difference."),
  answer("'makeResampleDesc' is much more flexible.", correct = TRUE),
  answer("Build in resample objects like 'cv10' can detect automatically if blocking or stratification is required."),
  answer("Blocking or stratification has to be defined within 'makeResampleDesc'.", correct = TRUE)
)
```

#### *(P)* Conduct a benchmark

It might be interesting to compare LDA, QDA, and naive Bayes on the iris task. Instead of calling three times the `resample()` function we now use `benchmark()` to compare multiple learner. Therefore, collect the learner to compare within a list and conduct the benchmark. Now, use just a 3-fold cross-validation to evaluate the performance:

```{r benchmark, exercise=TRUE, exercise.lines=10, exercise.checker=benchmarkChecker}
learners = list(...)
task =
res_desc =

set.seed(123)
bmr = benchmark(...)
```

```{r benchmark-hint-1}
# To compare multiple learner collect hem within a list
learners = list("classif.lda", "classif.qda", "classif.naiveBayes")
```

```{r benchmark-hint-2}
# Again, use the build in iris task or define your own
task = iris.task
```

```{r benchmark-hint-3}
# As resampling we know use the 3-fold cross validation
res_desc = cv3
res_desc = makeResampleDesc(method = "CV", iters = 3)
```

```{r benchmark-hint-4}
# Finally call benchmark
bmr = benchmark(learner = learners, tasks = task, resamplings = res_desc)
```

```{r benchmark-solution}
learners = list("classif.lda", "classif.qda", "classif.naiveBayes")
task = iris.task
res_desc = cv3

set.seed(123)
bmr = benchmark(learner = learners, tasks = task, resamplings = res_desc)
```

```{r benchmark-check}
learners = list("classif.lda", "classif.qda", "classif.naiveBayes")
task = iris.task
res_desc = cv3

set.seed(123)
bmr = benchmark(learner = learners, tasks = task, resamplings = res_desc)
```

#### *(P)* Visualize the benchmark

Finally, visualize the benchmark result with `plotBMRBoxplots()`

```{r bmr-viz, exercise=TRUE, exercise.lines=8}
set.seed(123)
bmr =

plotBMRBoxplots(bmr)
```

```{r bmr-viz-solution}
set.seed(123)
bmr = benchmark(learner = list("classif.lda", "classif.qda", "classif.naiveBayes"), tasks = iris.task, resamplings = cv10)

plotBMRBoxplots(bmr)
```

```{r bmr-viz-quiz, echo=FALSE}
question("What can you observe?",
  answer("The boxplots are drawn by taking the estimated performance of each fold.", correct = TRUE),
  answer("The median of LDA and QDA is zero.", correct = TRUE),
  answer("LDA seems to work best on the iris task.", correct = TRUE),
  answer("QDA and naive Bayes works equally good."),
  answer("The simplicity of LDA may be the trigger for the result.", correct = TRUE),
  answer("LDA is definitely the best model you can choose for that task.")
)
```