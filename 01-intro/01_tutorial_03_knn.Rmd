## K-Nearest-Neighbors

```{r, include=FALSE}
iris_task = makeClassifTask(data = iris[,c("Species", "Sepal.Width", "Petal.Width")], target = "Species")
```

**Study Goals**

- Learn how k-nearest-neighbors (k-NN) works
- Learn how to train a k-NN model using `mlr`

**Preparation**

1.  Watch the following video  (sorry, rather low volume...):
    <center>
    ![](https://www.youtube.com/watch?v=hXQq-lryqtE&list=PLMyWaJl2LoXyhFvMjtbBGs0Pi8khHbKm3&index=4&t=0s){width="75%"}
    </center>

2.  Read the [`mlr` tutorial about learners](https://mlr.mlr-org.com/articles/tutorial/learner.html) and how to [train](https://mlr.mlr-org.com/articles/tutorial/train.html) them.

```{r knn-quiz, echo=FALSE}
question("Which statements are true?",
  answer("The properties of k-NN are induced by the chosen distance metric.", 
         correct = TRUE),
  answer("k-NN can only be used for classification tasks."),
  answer("k-NN is an unsupervised learning method as it identifies groups of 
         observations $N_k(x)$ that belong together."),
  answer("$N_k(x)$ contains the subset of the feature space $\\mathcal X$ that 
         is at least as close to $x$ as the $k$-th closest neighbor of $x$ in
         the training data set."),
  answer("1-NN always 'predicts' perfectly on observations of the training data set.", 
         correct = TRUE),
  answer("k-NN with $k = n$ always predicts the same target variable value for
         all possible inputs $x$.",
         correct = TRUE),
  answer("The Gower distance between two observations which have different values
         for all features is always 1."),
  answer("The Gower distance between two observations which have the same values
         for all features is always 0.",
         correct = TRUE),
  answer("If you use the Euclidean distance, k-NN results do not change if you
         rescale the features."),
  answer("If you use the Gower distance, k-NN results do not change if you
         rescale the features.",
         correct = TRUE),
  answer("k-NN still performs well if most of the features we have are completely
         useless for predicting the target variable."),
  answer("The model representation of k-NN is simply the training data set.",
         correct = TRUE),
  answer("The size of the training data does not really affect the computational
         effort that is required to predict a new target variable value from a
         k-NN model."),
  answer("The size of $k$ does not really affect the computational effort that is
         required to predict a new target variable value from a k-NN model."),
  answer("The dimension $p$ of $x$ does not really affect the computational effort
         that is required to predict a new target variable value from a k-NN model.")
)
```

### Create the `mlr` learner

Create a k-NN learner with `k = 3` using the `kknn` function from the package `kknn` of the same name:
```{r kknn-definition, exercise=TRUE, exercise.lines=5, exercise.checker=checker}
kknn_learner =
```

```{r kknn-definition-check}
kknn_learner = makeLearner("classif.kknn", k = 3)
```


### Train the `mlr` learner

Now train the learner on the task defined in session 2:

```{r kknn-training, exercise=TRUE, exercise.lines=5, exercise.checker=checker}
kknn_model =
```

```{r, include=FALSE}
kknn_learner = makeLearner("classif.kknn", k = 3)
```

```{r kknn-training-check}
kknn_model = train(kknn_learner, iris_task)
```


### Visualize decision boundaries

Again, define the `kknn_learner` and visualize the prediction of the learner with `plotLearnerPrediction`. Rerun the code for different `k`. What can you observe by varying the hyperparameter?

```{r kknn-visualization, exercise=TRUE}
kknn_learner =
plotLearnerPrediction(kknn_learner, task = iris_task)
```
