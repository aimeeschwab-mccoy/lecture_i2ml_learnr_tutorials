## K-Nearest-Neighbors

```{r, include=FALSE}
iris_task = makeClassifTask(data = iris[,c("Species", "Sepal.Width", "Petal.Width")], target = "Species")
```

### Study Goals

*Theoretical (T)*

- Learn how k-nearest-neighbors (k-NN) works

*Practical (P)*

- Learn how to train a k-NN model using `mlr`

### Preparation

1.  Watch the following video  (sorry, rather low volume...):
    <center>
    ![](https://www.youtube.com/watch?v=hXQq-lryqtE&list=PLMyWaJl2LoXyhFvMjtbBGs0Pi8khHbKm3&index=4&t=0s){width="75%"}
    </center>

2.  Read the [`mlr` tutorial about learners](https://mlr.mlr-org.com/articles/tutorial/learner.html) and how to [train](https://mlr.mlr-org.com/articles/tutorial/train.html) them.

### Exercises

#### *(T)* Quiz

```{r knn-quiz, echo=FALSE}
question("Which statements are true?",
  answer("The properties of k-NN are induced by the chosen distance metric.",
         correct = TRUE),
  answer("k-NN can only be used for classification tasks."),
  answer("k-NN is an unsupervised learning method as it identifies groups of
         observations $N_k(x)$ that belong together."),
  answer("$N_k(x)$ contains the subset of the feature space $\\mathcal X$ that
         is at least as close to $x$ as the $k$-th closest neighbor of $x$ in
         the training data set."),
  answer("1-NN always 'predicts' perfectly on observations of the training data set.",
         correct = TRUE),
  answer("k-NN with $k = n$ always predicts the same target variable value for
         all possible inputs $x$.",
         correct = TRUE),
  answer("The Gower distance between two observations which have different values
         for all features is always 1."),
  answer("The Gower distance between two observations which have the same values
         for all features is always 0.",
         correct = TRUE),
  answer("If you use the Euclidean distance, k-NN results do not change if you
         rescale the features."),
  answer("If you use the Gower distance, k-NN results do not change if you
         rescale the features.",
         correct = TRUE),
  answer("k-NN still performs well if most of the features we have are completely
         useless for predicting the target variable."),
  answer("The model representation of k-NN is simply the training data set.",
         correct = TRUE),
  answer("The size of the training data does not really affect the computational
         effort that is required to predict a new target variable value from a
         k-NN model."),
  answer("The size of $k$ does not really affect the computational effort that is
         required to predict a new target variable value from a k-NN model."),
  answer("The dimension $p$ of $x$ does not really affect the computational effort
         that is required to predict a new target variable value from a k-NN model."),
  answer("The presence of irrelevant features doesn't affect the accuracy of k-NN."),
  answer("k-NN makes no assumptions about the underlying data distribution.",
         correct = TRUE),
  answer("Choosing an odd k can avoid ties in binary classification", correct = TRUE)
)
```

#### *(P)* Create the `mlr` learner

Create a k-NN learner with `k = 3` using the `kknn` function from the same named package `kknn`:
```{r kknn-definition, exercise=TRUE, exercise.lines=5, exercise.checker=createChecker("kknn_learner")}
kknn_learner =
```

```{r kknn-definition-hint-1, eval=FALSE}
# Use the 'makeLearner' function of mlr
makeLearner(...)
```

```{r kknn-definition-hint-2}
# Use the 'classif.kknn' learner
"classif.kknn"
```

```{r kknn-definition-hint-3}
# You can specify how many neighbors by setting 'k'
```

```{r kknn-definition-solution}
makeLearner("classif.kknn", k = 3)
```

```{r kknn-definition-check}
kknn_learner = makeLearner("classif.kknn", k = 3)
```


#### *(P)* Train the `mlr` learner

```{r, include=FALSE}
iris_task = makeClassifTask(data = iris[,c("Species", "Sepal.Width", "Petal.Width")], target = "Species")
kknn_learner = makeLearner("classif.kknn", k = 3)
```

Now train the learner `kknn_learner` defined above on the task `iris_task` defined in session 2 and
save the learnt model as `kknn_model`:

```{r kknn-training, exercise=TRUE, exercise.liens=5, exercise.checker=createChecker("kknn_model")}
kknn_model =
```

```{r kknn-training-hint-1}
# Use the train function of mlr
train(...)
```

```{r kknn-training-hint-2}
# Just pass the 'kknn_learner' and the 'iris_task' to train
```


```{r kknn-training-solution}
train(kknn_learner, iris_task)
```

```{r kknn-training-check}
kknn_model = train(kknn_learner, iris_task)
```


#### *(P)* Visualize decision boundaries

Again, define the `kknn_learner` and visualize the prediction of the learner with `plotLearnerPrediction`. Rerun the code for different `k`. What can you observe by varying the hyperparameter?

```{r kknn-visualization, exercise=TRUE}
kknn_learner =
plotLearnerPrediction(kknn_learner, task = iris_task)
```
