## Overfitting

### Study Goals

*Theoretical (T)*

- Learn what overfitting means
- Get familiar with simple performance measures for regression and classification

*Practical (P)*

- See how k-NN overfits for small $k$

### Preparation

1.  *(T)* Watch the following videos (sorry, rather low volume...):
    <center>
    ![](https://youtu.be/-S5h9ayrXh4){width="75%"}
    </center>
    <a href="https://github.com/compstat-lmu/lecture_i2ml/blob/master/slides-pdf/slides-overfitting.pdf" target="_blank">Slideset</a>

1. *(P)* You should have done the [tutorial about k-NN](https://compstat-lmu.shinyapps.io/01_tutorial/#section-k-nearest-neighbors)

### Exercises

#### *(T)* Quiz

```{r overfitting-quiz, echo=FALSE}
# quiz(caption = "Basic Terminology",
  question("Which statements are true?",
    answer("Overfitting is more likely with higher complexity of the model.", correct = TRUE),
    answer("Overfitting means that the model performs very well on the training data."),
    answer("Overfitting means that the model performs much better on the training data than on the test data.", correct = TRUE),
    answer("A good test performance is an indicator of overfitting."),
    answer("The linear model is well known to overfit very fast."),
    answer("Constraining the hypothesis space helps the learner to find a good hypothesis.")
)
```

#### *(P)* Overfitting k-NN

To visually see how the number of nearest neighbors affects the overfitting behavior we simulate data as followed:

1.  Draw 100 observations of a binary target variable with equal distributed classes `1` and `2`.
    ```{r, eval=FALSE}
    class = sample(c(1, 2), size = 100, replace = TRUE)
    ```

1. Simulate data with two normally distributed features `x` and `y` and a target variable `class`:
    * Class `1` should have a mean of $2$ and standard deviation of $1$
    * Class `2` should have a mean of $4$ and standard deviation of $2$
    * `class` should contain the classes as factor or character variable
    ```{r, eval=FALSE}
    df_sim = data.frame(x = rnorm(100, mean = 2*class, sd = class), y = rnorm(100, mean = 2*class, sd = class), class = as.character(class))
    ```

1.  Generate a task out of the simulated data with `target = "class"` and define the k-NN learner with `k = 1`:
    ```{r, eval=FALSE}
    task_sim = makeClassifTask(data = df_sim, target = "class")
    kknn_learner = makeLearner("classif.kknn", k = 1)
    ```

1.  Finally, call `plotLearnerPrediction()` on the learner and task:
    ```{r, eval=FALSE}
    plotLearnerPrediction(learner = kknn_learner, task = task_sim)
    ```

Now, put all together and vary `k` to see how the overfitting behavior of k-NN reacts on the choice of `k`:

```{r kknn-overfitting, exercise=TRUE}
set.seed(123)
class =
df_sim =

task_sim =
kknn_learner =

plotLearnerPrediction(learner = ..., task = ...)
```

```{r kknn-overfitting-solution}
set.seed(123)
class = sample(c(1, 2), size = 100, replace = TRUE)
df_sim = data.frame(x = rnorm(100, mean = 2*class, sd = class), y = rnorm(100, mean = 2*class, sd = class), class = as.character(class))

task_sim = makeClassifTask(data = df_sim, target = "class")
kknn_learner = makeLearner("classif.kknn", k = 1)

plotLearnerPrediction(learner = kknn_learner, task = task_sim)
```
