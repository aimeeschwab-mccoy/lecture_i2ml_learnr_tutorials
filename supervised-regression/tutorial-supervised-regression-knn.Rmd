## K-Nearest-Neighbors

### Study Goals

*Theoretical (T)*

- Learn how k-nearest-neighbors (k-NN) works

*Practical (P)*

- Learn how to train a k-NN model using `mlr3`

### Preparation

1.  *(T)* Watch the following video:
    <center>
    ![k-NN](https://youtu.be/g8H6-MkN_q0){width="75%"}
    </center>

2.  Read the `mlr3` tutorial about [learners](https://mlr3book.mlr-org.com/learners.html) and how to [train](https://mlr3book.mlr-org.com/train-predict.html) them.


### Exercises

#### *(T)* Quiz

```{r knn-quiz, echo=FALSE}
question("Which statements are true?",
  answer("The properties of k-NN are induced by the chosen distance metric.",
         correct = TRUE),
  answer("k-NN can only be used for classification tasks."),
  answer("$N_k(x)$ contains the subset of the feature space $\\mathcal X$ that
         is at least as close to $x$ as the $k$-th closest neighbor of $x$ in
         the training data set.", correct = TRUE),
  answer("1-NN always 'predicts' perfectly on observations of the training data set.",
         correct = TRUE),
  answer("k-NN with $k = n$ always predicts the same target variable value for
         all possible inputs $x$.",
         correct = TRUE),
  answer("The Gower distance between two observations which have different values
         for all features is always 1."),
  answer("The Gower distance between two observations which have the same values
         for all features is always 0.",
         correct = TRUE),
  answer("If you use the Euclidean distance, k-NN results do not change if you
         rescale the features."),
  answer("If you use the Gower distance, k-NN results do not change if you
         rescale the features.",
         correct = TRUE),
  # answer("k-NN still performs well if most of the features we have are completely
  #        useless for predicting the target variable."),
  # answer("The model representation of k-NN is simply the training data set.",
  #        correct = TRUE),
  # answer("The size of the training data does not really affect the computational
  #        effort that is required to predict a new target variable value from a
  #        k-NN model."),
  # answer("The size of $k$ does not really affect the computational effort that is
         # required to predict a new target variable value from a k-NN model."),
  # answer("The dimension $p$ of $x$ does not really affect the computational effort
  #        that is required to predict a new target variable value from a k-NN model."),
  answer("The presence of irrelevant features doesn't affect the accuracy of k-NN."),
  answer("k-NN makes no assumptions about the underlying data distribution.",
         correct = TRUE)
  # answer("Choosing an odd k can avoid ties in binary classification", correct = TRUE)
)
```

#### *(P)* Create the `mlr3` learner

<!-- TODO: example should be regression, not classification -->

Create a k-NN learner with `k = 3` using the `kknn` function from the same named package `kknn`:
```{r kknn-definition, exercise=TRUE, exercise.lines=5, exercise.checker=learnerChecker("kknn_learner")}
kknn_learner <-
```

```{r kknn-definition-hint-1, eval=FALSE}
# Use the 'lrn()' function of mlr
lrn(...)
```

```{r kknn-definition-hint-2}
# Use the 'classif.kknn' learner
"classif.kknn"
```

```{r kknn-definition-hint-3}
# You can specify how many neighbors by setting 'k'
```

```{r kknn-definition-solution}
kknn_learner <- lrn("classif.kknn", k = 3)
```

```{r kknn-definition-check}
kknn_learner <- lrn("classif.kknn", k = 3)
```


#### *(P)* Train the `mlr3` learner

Now train the learner `kknn_learner` defined above on the task `iris_task` defined in session 2:

```{r kknn-training, exercise=TRUE, exercise.liens=5, exercise.checker=modelChecker("kknn_learner")}
iris_task <-
kknn_learner <-

kknn_learner$
```

```{r kknn-training-hint-1}
# Use iris_task and kknn_learner from above
iris_task <- TaskClassif$new(id = "iris_task", 
                             backend = iris[,c("Species", "Sepal.Width", "Petal.Width")], 
                             target = "Species")
kknn_learner <- lrn("classif.kknn", k = 3)
```

```{r kknn-training-hint-2}
# Use the class method 'train()' of the learner object
kknn_learner$train(...)
```

```{r kknn-training-hint-3}
# Just pass the 'iris_task' to train the 'kknn_learner'
```


```{r kknn-training-solution}
iris_task <- TaskClassif$new(id = "iris_task", 
                             backend = iris[,c("Species", "Sepal.Width", "Petal.Width")], 
                             target = "Species")
kknn_learner <- lrn("classif.kknn", k = 3)

kknn_learner$train(iris_task)
```

```{r kknn-training-check}
iris_task <- TaskClassif$new(id = "iris_task", 
                             backend = iris[,c("Species", "Sepal.Width", "Petal.Width")], 
                             target = "Species")
kknn_learner <- lrn("classif.kknn", k = 3)

kknn_learner$train(iris_task)
```


#### *(P)* Visualize decision boundaries

Again, define the `kknn_learner` and visualize the prediction of the learner with `plot_prediction()`. Rerun the code for different `k`. What can you observe by varying the hyperparameter?

```{r kknn-visualization, exercise=TRUE}
iris_task <-
kknn_learner <-

plot_prediction(learner = kknn_learner, task = iris_task)
```

```{r kknn-visualization-solution}
iris_task <- TaskClassif$new(id = "iris_task", 
                             backend = iris[,c("Species", "Sepal.Width", "Petal.Width")], 
                             target = "Species")
kknn_learner <- lrn("classif.kknn", k = 3)

plot_learner_prediction(learner = kknn_learner, task = iris_task)
```
